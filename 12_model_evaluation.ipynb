{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Evaluation\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model development process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rigorous approach to model development uses both cross-validation and validation. The cross-validation can be used to tune hyperparameters, while the separate validation set lets you compare the scores of different algorithms (e.g., logistic regression vs. Naive Bayes vs. decision tree) to select a champion model. Finally, the test set gives a benchmark score for performance on new data. This process is illustrated in the diagram below.\n",
    "\n",
    "![model_development_diagram](images/model_development_diagram.png)\n",
    "\n",
    "*image source: google advanced data analytics certificate*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metrics used to evaluate regression models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric | Perfect Score | Description |\n",
    "| --- | --- | --- |\n",
    "| R-squared (R^2) | 1 | Proportion of the variance in the dependent variable (Y) explained by the independent variables (X). Range of 0 to 1, where 1 represents a perfect fit of the model to the data. |\n",
    "| Adjusted R-squared | 1 | Modified R^2 accounts for the number of predictors in order to avoid overfitting. Does this by penalizing unnecessary explanatory variables. Range of 0 to 1, where 1 represents a perfect fit of the model to the data. |\n",
    "| Mean Squared Error (MSE) | 0 | Measures the average squared difference between predicted and actual values. |\n",
    "| Root Mean Squared Error (RMSE) | 0 | Square root of MSE, providing a measure of the average magnitude of errors. |\n",
    "| Mean Absolute Error (MAE) | 0 | Measures the average absolute difference between predicted and actual values. |\n",
    "| Mean Percentage Error (MPE) | 0 | Measures the average percentage difference between predicted and actual values. |\n",
    "| Mean Absolute Percentage Error (MAPE) | 0 | Measures average absolute percentage difference between predicted and actual values. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the \"ideal\" score for each metric is task-dependent and may vary based on the specific problem context. For example, for some metrics like MSE, RMSE, and MAE, lower values indicate better performance, while for others like R-squared, higher values (closer to 1) indicate a better fit of the model to the data.\n",
    "\n",
    "The R-squared (R2) and Adjusted R-squared metrics have a range of 0 to 1, where 1 represents a perfect fit of the model to the data.\n",
    "\n",
    "Metrics like Mean Percentage Error (MPE) and Mean Absolute Percentage Error (MAPE) are expressed as percentages. The perfect score for these metrics is 0, indicating no percentage difference between the predicted and actual values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics used to evaluate classification models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric | Ideal Score | Description | Appropriate When... |\n",
    "| --- | --- | --- | --- |\n",
    "| Accuracy | 1 | Measures the overall correctness of the model's predictions, the proportion of correct classifications. | Classes are balanced or misclassification costs are equal across classes. |\n",
    "| Precision | 1 | Measures the proportion of correctly predicted positive instances out of the total predicted positive instances. | The cost of false positives is high (e.g., spam detection). |\n",
    "| Recall (Sensitivity) | 1 | Measures the proportion of correctly predicted positive instances out of the total actual positive instances. | The cost of false negatives is high (e.g., disease detection). |\n",
    "| F1 Score | 1 | Harmonic mean of precision and recall. Provides a balanced measure of model performance. | Balance between precision and recall is desired (e.g., text classification). |\n",
    "| Area Under the ROC Curve (AUC) | 1 | Represents the probability that a randomly selected positive instance is ranked higher than a randomly selected negative instance according to the model's predictions. | The class distribution is imbalanced or when ranking predictions is important.\n",
    "Ranking predictions refers to the ability of a classification model to correctly order or rank instances according to their predicted probabilities of belonging to a particular class. |\n",
    "| Confusion Matrix | N/A | A table representing the performance of a classification model. It shows true positive, true negative, false positive, and false negative values. | Comprehensive understanding of the model's performance across different classes is needed. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to consider the specific requirements, goals, and costs associated with the classification problem when selecting the appropriate evaluation metric. Depending on the context, different performance measures may be favored. For example, in scenarios where the cost of false positives is high (e.g., in medical diagnosis), precision is a crucial metric. On the other hand, in situations where the cost of false negatives is high (e.g., in fraud detection), recall becomes more important."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Classification model metric evaluation throughout the train, validate, test process.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Define dictionary of scoring metrics (req. for GridSearchCV object instantiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step.1 - Training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Define a function to generate scores on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_results(model_name:str, model_object, metric:str):\n",
    "    '''\n",
    "    Arguments:\n",
    "        model_name (string): what you want the model to be called in the output table\n",
    "        model_object: a fit GridSearchCV object\n",
    "        metric (string): precision, recall, f1, or accuracy\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean 'metric' score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
    "    metric_dict = {'precision': 'mean_test_precision',\n",
    "                 'recall': 'mean_test_recall',\n",
    "                 'f1': 'mean_test_f1',\n",
    "                 'accuracy': 'mean_test_accuracy',\n",
    "                 'auc': 'mean_test_roc_auc'\n",
    "                 }\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(metric) score\n",
    "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
    "\n",
    "    # Extract Accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    auc = best_estimator_results.mean_test_roc_auc\n",
    "  \n",
    "    # Create table of results\n",
    "    # table = pd.DataFrame()\n",
    "    table = pd.DataFrame({'Model': model_name,\n",
    "                        'Precision': precision,\n",
    "                        'Recall': recall,\n",
    "                        'F1': f1,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'AUC': auc,\n",
    "                        },\n",
    "                        index=[0]\n",
    "                    )\n",
    "\n",
    "    return table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Get scores on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call 'make_results()' on the GridSearch object\n",
    "results = get_training_results('random forest 1: f1', rf_val, 'auc')\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step.2 - Validating\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Define a function to generate scores on validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_validation_scores(model_name:str, preds, y_val_data):\n",
    "    '''\n",
    "    Generate a table of validation scores.\n",
    "\n",
    "    In: \n",
    "        model_name (string): Your choice: how the model will be named in the output table\n",
    "        preds: numpy array of validation predictions\n",
    "        y_test_data: numpy array of y_val data\n",
    "\n",
    "    Out: \n",
    "        table: a pandas df of precision, recall, f1, and accuracy, auc scores for your model\n",
    "    '''\n",
    "    accuracy = round(accuracy_score(y_val_data, preds), 3)\n",
    "    precision = round(precision_score(y_val_data, preds), 3)\n",
    "    recall = round(recall_score(y_val_data, preds), 3)\n",
    "    f1 = round(f1_score(y_val_data, preds), 3)\n",
    "    auc = round(roc_auc_score(y_val_data, preds), 3)\n",
    "\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                        'precision': [precision], \n",
    "                        'recall': [recall],\n",
    "                        'f1': [f1],\n",
    "                        'accuracy': [accuracy],\n",
    "                        'AUC': [auc]\n",
    "                        })\n",
    "    return table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Get scores on validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_validation_scores = get_validation_scores('model_name', preds, y_val)\n",
    "model_validation_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step.3 - Testing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Define a function to generate scores on validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_test_scores(model_name:str, preds, y_test_data):\n",
    "    '''\n",
    "    Generate a table of test scores.\n",
    "\n",
    "    In: \n",
    "        model_name (string): Your choice: how the model will be named in the output table\n",
    "        preds: numpy array of test predictions\n",
    "        y_test_data: numpy array of y_test data\n",
    "\n",
    "    Out: \n",
    "        table: a pandas df of precision, recall, f1, and accuracy scores for your model\n",
    "    '''\n",
    "    accuracy = round(accuracy_score(y_test_data, preds), 3)\n",
    "    precision = round(precision_score(y_test_data, preds), 3)\n",
    "    recall = round(recall_score(y_test_data, preds), 3)\n",
    "    f1 = round(f1_score(y_test_data, preds), 3)\n",
    "    auc = round(roc_auc_score(y_test_data, preds), 3)\n",
    "\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                        'precision': [precision], \n",
    "                        'recall': [recall],\n",
    "                        'f1': [f1],\n",
    "                        'accuracy': [accuracy],\n",
    "                        'AUC': [auc]\n",
    "                        })\n",
    "  \n",
    "    return table\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Get scores on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get final test scores for the selected model\n",
    "results = get_test_scores('random forest: auc', preds, y_test)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
