{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xSDP4bMrIL2x"
   },
   "source": [
    "# Gradient Boosting\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   **[Introduction to Gradient Boosting](#1.-Introduction-to-Gradient-Boosting)**\n",
    "2.   **[Foundations of Gradient Boosting](#2.-Foundations-of-Gradient-Boosting)**\n",
    "3.   **[Gradient Boost Tuning](#3.-Gradient-Boost-Tuning)**\n",
    "4.   **[Model Validation](#4.-Model-Validation)**\n",
    "5.   **[Exploratory Data Analysis](#5.-Exploratory-Data-Analysis)**\n",
    "6.   **[Model Construction](#6.-Model-Construction)**\n",
    "7.   **[Model Evaluation](#7.-Model-Evaluation)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"1.-Introduction-Gradient-Boosting\"></a>\n",
    "### 1. Introduction to Gradient Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting |** Technique that builds an ensemble of weak learners sequentially, with each consecutive learner trying to correct the errors of the one that preceded it\n",
    "\n",
    "Comparison to bagging:\n",
    "\n",
    "Similarities:\n",
    "- Ensemble technique\n",
    "- Aggregates weak learners\n",
    "\n",
    "Differences: \n",
    "- Learners are built sequentially, not in parallel\n",
    "- Not limited to tree-based learners\n",
    "\n",
    "**Adaptive Boosting |** A boosting methodology where each consecutive base learner assigns greater weight to the observations incorrectly predicted by the preceding learner\n",
    "\n",
    "**Gradient Boosting |** A boosting methodology where each base learner in the sequence is built to predict the residual errors of the model that precede it\n",
    "\n",
    "**Black-box model |** Any model whose predictions cannot be precisely explained\n",
    "\n",
    "**Extrapolation |** A model's ability to predict new values that fall outside of the range of values in the training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"2.-Foundations-of-Gradient-Boosting-Machines\"></a>\n",
    "### 2. Foundations of Gradient Boosting Machines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Concept\n",
    "GBMs are model ensembles that use gradient boosting. Gradient boosting is one of the most powerful supervised learning techniques. Although GBMs do not have to be tree-based, tree ensembles are the most common implementation of this technique. There are two key features of tree-based gradient boosting that set it apart from other modeling techniques:\n",
    "\n",
    "1. It works by building an ensemble of decision tree base learners wherein each base learner is trained successively, attempts to predict the error—also known as “residual”—of the previous tree, and therefore compensate for it.\n",
    "2. Its base learner trees are known as “weak learners” or “decision stumps.” They are generally very shallow.\n",
    "\n",
    "##### 2.1.2 Advantages\n",
    "- High accuracy\n",
    "- Generally scalable\n",
    "- Work well with missing data\n",
    "- Don't require parameter scaling\n",
    "\n",
    "##### 2.1.2 Disadvantages\n",
    "- Tuning many hyperparameters can be time-consuming\n",
    "- Difficult to interpret\n",
    "- Have difficulty with extrapolation\n",
    "- Prone to overfitting if too many hyperparameters are tuned\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"3.-Gradient-Model-Tuning\"></a>\n",
    "### 3. Gradient Model Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 3.1 Models\n",
    "\n",
    "For classification tasks:\n",
    "\n",
    "**from xgboost import XGBClassifier**\n",
    "\n",
    "For regression tasks:\n",
    "\n",
    "**from xgboost import XGBRegressor**\n",
    "\n",
    "#### 3.2 Most Important Hyperparameters for Gradient Model Tuning:\n",
    "\n",
    "| Hyperparameter | What it does | Input type | Default Value | Considerations |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| `n_estimators` | Specifies the number of trees the model will build in its ensemble | int | 100 | A typical range is 50–500. Consider how much data, how deep the trees are allowed to grow and how many samples are bootstrapped to grow each tree (generally need more trees if they’re shallow, and more trees if your bootstrap sample size is smaller). Also consider that gradient models grow trees sequentially so training can take much longer than random forest models. |\n",
    "| `max_depth` | Specifies how many levels tree can have.   If None, trees grow until leaves are pure or until all leaves contain less than min_child_weight. | int | 3  | Controls complexity of the model. Gradient boosting typically uses weak learners, or “decision stumps” (i.e., shallow trees). Restricting tree depth can reduce training times and serving latency as well as prevent overfitting. Consider values 2–6 |\n",
    "| `min_child_weight` | Controls threshold below which a node becomes a leaf, based on the combined weight of the samples it contains. For regression models, this value is functionally equivalent to a number of samples. For the binary classification objective, the weight of a sample in a node is dependent on its probability of response as calculated by that tree. The weight of the sample decreases the more certain the model is (i.e., the closer the probability of response is to 0 or 1). | int or float | 1 | Higher values will stop trees splitting further, and lower values will allow trees to continue to split further. If your model is underfitting, then you may want to lower it to allow for more complexity. Conversely, increase this value to stop your trees from getting too finely divided. |\n",
    "| `learning_rate` | Controls how much importance is given to each consecutive base learner in the ensemble’s final prediction. Also known as eta or shrinkage. | float | 0.1 | Values can range from (0–1]. Typical values range from 0.01 to 0.3. Lower values mean less weight is given to each consecutive base learner. Consider how many trees are in your ensemble. Lower values typically benefit from more trees.\n",
    "| `colsample_bytree` | Specifies the percentage (0–1.0] of features that each tree randomly selects during training | float | 1.0 | Adds randomness to the model to make it robust to noise. Consider how many features the dataset has and how many trees will be grown. Fewer features sampled means more base learners might be needed. Small colsample_bytree values on datasets with many features mean more unpredictive trees in the ensemble\n",
    "| `subsample` | Specifies the percentage (0–1.0] of observations sampled from the dataset to train each base model. | float | 1.0 | Adds randomness to the model to make it robust to noise. Consider the size of dataset. When working with large datasets, it can be beneficial to limit the number of samples in each tree, because doing so can greatly reduce training time and still result in a robust model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"4.-Model-Validation\"></a>\n",
    "### 4. Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model validation is the set of processes and activities intended to verify that models are performing as expected. Model validation refers to the whole process of evaluating different models, selecting one, and then continuing to analyze the performance of the selected model to better understand its strengths and limitations. \n",
    "\n",
    "#### 4.1 Validation Sets \n",
    "The simplest way to maintain the objectivity of the test data is to create another partition in the data—a validation set—and save the test data for after you select the final model. The validation set is then used, instead of the test set, to compare different models.\n",
    "\n",
    "When building a model using a separate validation set, once the final model is selected, best practice is to go back and fit the selected model to all the non-test data (i.e., the training data + validation data) before scoring this final model on the test data.\n",
    "\n",
    "\n",
    "#### 4.2 Cross Validation \n",
    "Cross Validation is a process that uses different portions of the data to test and train a model on different iterations.  \n",
    "\n",
    "Cross-validation makes more efficient use of the training data by splitting the training data into k number of “folds” (partitions), training a model on k – 1 folds, and using the fold that was held out to get a validation score. The training process occurs k times, each time using a different fold as the validation set. At the end, the final validation score is the average of all k scores. This process is also commonly referred to as k-fold cross validation.\n",
    "\n",
    "After a model is selected using cross-validation, that selected model is then refit to the entire training set (i.e., it’s retrained on all k folds combined).\n",
    "\n",
    "Cross-validation reduces the likelihood of significant divergence of the distributions in the validation data from those in the full dataset. For this reason, it’s often the preferred technique when working with smaller datasets, which are more susceptible to randomness. The more folds you use, the more thorough the validation. However, adding folds increases the time needed to train, and may not be useful beyond a certain point.\n",
    "\n",
    "#### 4.3 Model Selection\n",
    "Once candidate models have been trained and validated a champion model is selected. Model validation scores factor heavily into this selection but score is seldom the only criterion. Often other factors are also considered. \n",
    "- How explainable is the model? \n",
    "- How complex is it? \n",
    "- How resilient is it against fluctuations in input values? \n",
    "- How well does it perform on data not found in the training data? \n",
    "- How much computational cost does it have to make predictions? \n",
    "- Does it add much latency to any production system? \n",
    "It’s not uncommon for a model with a slightly lower validation score to be selected over the highest-scoring model due to it being simpler, less computationally expensive, or more stable.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"5.-Exploratory-Data-Analysis\"></a>\n",
    "### 5. Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and modules.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Load the dataset into a DataFrame and save in a variable\n",
    "data = pd.read_csv(\"example_file.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Data Exploration\n",
    "After loading the dataset, the next step is to prepare the data to be suitable for clustering. This includes: \n",
    "\n",
    "*   Exploring data\n",
    "*   Checking for missing values\n",
    "*   Encoding categorical data \n",
    "*   Dropping irrelevant columns\n",
    "*   Renaming columns\n",
    "*   Create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows, number of columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data type for each column. NB logistic regression models expect numeric data\n",
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to answer:** Identify the target (or predicted) variable. What is the initial hypothesis about which variables will be valuable in predicting the target variable?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 Prepare model for predictions\n",
    "\n",
    "**Question to answer:** Before proceeding with modeling, consider which metrics should ultimately be leveraged to evaluate the model. Which metrics are most suited to evaluating this type of model?\n",
    "- Important to evaluate not just accuracy but the balance of false positives and false negatives that the model predicts. Therefore precision, recall and f1 score will be the best metrics for classification models.\n",
    "- The ROC AUC score is also suited to classification modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.1 Convert Variables to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the object predictor variables to numerical dummies.\n",
    "data_dummies = pd.get_dummies(data, \n",
    "                                columns=['categorical_column1','categorical_column2','categorical_column3','categorical_column4'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.2 Isolate Target and Predictor Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset into labels (y) and features (X).\n",
    "y = data_subset[\"target_variable\"]\n",
    "\n",
    "X = data_subset.copy()\n",
    "X = X.drop(\"target_variable\", axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.4 Create Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into train, validate, test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify= y, random_state = 0)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.25, stratify= y_train, random_state = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"6.-Model-Construction\"></a>\n",
    "### 6. Model Construction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Instantiate XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define xgb to be your XGBClassifier.\n",
    "xgb = XGBClassifier(random_state = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Define Hyperparameters for Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for tuning as `cv_params`\n",
    "cv_params = {'max_depth': [4, 6],\n",
    "              'min_child_weight': [3, 5],\n",
    "              'learning_rate': [0.1, 0.2, 0.3],\n",
    "              'n_estimators': [5,10,15],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7]\n",
    "              }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Define Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criteria as `scoring`\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Construct GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the GridSearch.\n",
    "xgb_cv = GridSearchCV(xgb, cv_params, scoring = scoring, cv= 4, refit = 'f1', n_jobs= -1, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit the GridSearch model to training data\n",
    "xgb_cv = xgb_cv.fit(X_train, y_train)\n",
    "xgb_cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to answer:** Which optimal set of parameters did the GridSearch yield?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Save Model for Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `pickle` to save the trained model.\n",
    "pickle.dump(xgb_cv, open('xgb_cv.sav', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"7.-Model-Evaluation\"></a>\n",
    "### 7. Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Formulate Predictions on Test Set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the predictions yielded from model, leverage a series of metrics and evaluation techniques from scikit-learn by examining the actual observed values in the test set relative to model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model to predict on test data\n",
    "y_pred = xgb_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print your accuracy score.\n",
    "ac_score = metrics.accuracy_score(y_test, y_pred)\n",
    "print('accuracy score:', ac_score)\n",
    "\n",
    "# 2. Print your precision score.\n",
    "pc_score = metrics.precision_score(y_test, y_pred)\n",
    "print('precision score:', pc_score)\n",
    "\n",
    "# 3. Print your recall score.\n",
    "rc_score = metrics.recall_score(y_test, y_pred)\n",
    "print('recall score:', rc_score)\n",
    "\n",
    "# 4. Print your f1 score.\n",
    "f1_score = metrics.f1_score(y_test, y_pred)\n",
    "print('f1 score:', f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** When observing the precision and recall scores of your model, how do you interpret these values, and is one more accurate than the other?\n",
    "\n",
    "**Question:** What does your model's F1 score tell you, beyond what the other metrics provide?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Confusion Matrix for Clarity\n",
    "A confusion matrix is a graphic that shows a model's true and false positives and true and false negatives. It helps to create a visual representation of the components feeding into the metrics above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct and display confusion matrix.\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create the display for confusion matrix\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb_cv.classes_)\n",
    "\n",
    "# Plot the visual in-line\n",
    "disp.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to answer:** When observing the confusion matrix, what do you notice? Does this correlate to any of your other calculations?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Visualize Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relative feature importance of the predictor variables in your model\n",
    "plot_importance(xgb_cv.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to answer:** Examine the feature importance outputted above. What is your assessment of the result? Did anything surprise you?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of results to compare model performance\n",
    "table = pd.DataFrame()\n",
    "table = table.append({'Model': \"Tuned Decision Tree\",\n",
    "                        'F1':  0.945422,\n",
    "                        'Recall': 0.935863,\n",
    "                        'Precision': 0.955197,\n",
    "                        'Accuracy': 0.940864\n",
    "                      },\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "\n",
    "table = table.append({'Model': \"Tuned Random Forest\",\n",
    "                        'F1':  0.947306,\n",
    "                        'Recall': 0.944501,\n",
    "                        'Precision': 0.950128,\n",
    "                        'Accuracy': 0.942450\n",
    "                      },\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "\n",
    "table = table.append({'Model': \"Tuned XGBoost\",\n",
    "                        'F1':  f1_score,\n",
    "                        'Recall': rc_score,\n",
    "                        'Precision': pc_score,\n",
    "                        'Accuracy': ac_score\n",
    "                      },\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to answer:** How does this model compare to the other models that have been built to solve the same problem\n",
    "\n",
    "**Sharing findings:**\n",
    "\n",
    "- Showcase the data used to create the prediction and the performance of the model overall.\n",
    "- Review the sample output of the features and the confusion matrix to reference the model's performance.\n",
    "- Highlight the metric values, emphasizing the F1 score.\n",
    "- Visualize the feature importance to showcase what drove the model's predictions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1BgN3Lv1fx-AxyKSqB_2kM3dJ4mFBctv_",
     "timestamp": 1662734078308
    },
    {
     "file_id": "1ZYfhIvPRxnw7ghB_BsNQAMUorLXpAZs_",
     "timestamp": 1658889786811
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
